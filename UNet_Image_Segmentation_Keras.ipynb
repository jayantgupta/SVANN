{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "@author: Jayant Gupta\"\n",
    "last update: 02.06.2021\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==================== Import necessary libraries ==================== \n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from models import *\n",
    "from generators import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from skimage.io import imread, imshow, show\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Handles reading large number of files.\n",
    "Image.MAX_IMAGE_PIXELS = 219494175 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================== Generate dataset X-Y filepath pairs ==================== \n",
    "seed = 42\n",
    "np.random.seed = seed\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "IMG_HEIGHT = 1024\n",
    "IMG_WIDTH = 1024\n",
    "IMG_CHANNELS = 3\n",
    "IMG_INDEX = \"RGB\"\n",
    "\n",
    "IN_FOLDERS = [\"MN_raster_Hennepin_South\"]\n",
    "\n",
    "# This function creates a dictionary to store images and their mask.\n",
    "# This function is curated to handle train images *.tif format and \n",
    "# mask file in *.jpeg format\n",
    "def get_in_out_dict(img_dir=json.load(open('config.json'))['filepaths']['default_reprojections_dir'], \n",
    "                    mask_dir=json.load(open('config.json'))['filepaths']['default_masks_dir'], \n",
    "                    indices_dir=None, \n",
    "                    index_type=None,\n",
    "                    in_folder=''):\n",
    "    in_out_dict = []\n",
    "    img_path = os.path.join(img_dir, in_folder) # img_dir/in_folder\n",
    "    for img_filename in os.listdir(img_path):\n",
    "        in_file = os.path.join(img_path, img_filename) # img_dir/in_folder/img_filename\n",
    "        if os.path.isdir(in_file):\n",
    "            if IMG_CHANNELS > 3:\n",
    "                in_out_dict += get_in_out_dict(indices_dir=indices_dir, index_type=IMG_INDEX, in_folder=os.path.join(in_folder, img_filename))\n",
    "            else:\n",
    "                in_out_dict += get_in_out_dict(in_folder=os.path.join(in_folder, img_filename))\n",
    "        if in_file.endswith(\".tif\") is not True:\n",
    "            continue\n",
    "        filename = img_filename.split('.')[0]\n",
    "        filenames = []\n",
    "        filenames.append(in_file)\n",
    "        if IMG_CHANNELS > 3:\n",
    "            indices_path = os.path.join(indices_dir, in_folder) # indices_dir/in_folder\n",
    "            filenames.append(\n",
    "                os.path.join(indices_path, [index for index in os.listdir(indices_path) if filename in index and Index_Type in index][0]))\n",
    "        mask_path = os.path.join(mask_dir, in_folder) # mask_path/in_folder\n",
    "        filenames.append(\n",
    "            os.path.join(mask_path, [mask for mask in os.listdir(mask_path) if filename in mask][0]))\n",
    "        in_out_dict.append(filenames)\n",
    "\n",
    "    return in_out_dict\n",
    "\n",
    "in_out_dict=[]\n",
    "for in_folder in IN_FOLDERS:\n",
    "    if IMG_CHANNELS > 3:\n",
    "        indices_dir = json.load(open('config.json'))['filepaths']['default_indices_dir']\n",
    "        in_out_dict += get_in_out_dict(in_folder=in_folder, indices_dir=indices_dir, index_type=IMG_INDEX)\n",
    "    else:\n",
    "        in_out_dict += get_in_out_dict(in_folder=in_folder)\n",
    "\n",
    "in_out_dict = np.array(in_out_dict)\n",
    "X_in = in_out_dict[:,0]\n",
    "Y_in = in_out_dict[:,1]\n",
    "print(X_in.shape, Y_in.shape)\n",
    "print(X_in[0:5], Y_in[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================== Read filepath pairs into dataset X-Y pairs ==================== \n",
    "#def join_image_and_index(img_path, index_path):\n",
    "#    image = Image.open(img_path) # Using Pillow Image object\n",
    "#    image = image.resize((IMG_HEIGHT, IMG_WIDTH)) # \n",
    "#    image_array = np.asarray(image)/255.\n",
    "#\n",
    "#    index = Image.open(index_path) # Using Pillow Image object\n",
    "#    index = index.resize((IMG_HEIGHT, IMG_WIDTH))\n",
    "#    index_array = np.asarray(index)\n",
    "#    return np.dstack((image_array, index_array))\n",
    "#\n",
    "## Function to read the data from image_mask_pair file paths.\n",
    "#def read_data(in_out_dict):\n",
    "#    X_train = np.zeros((len(in_out_dict), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n",
    "#    Y_train = np.zeros((len(in_out_dict), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "#\n",
    "#    keys = in_out_dict.keys()\n",
    "#\n",
    "#    print('Resizing training images and masks')\n",
    "#    n = 0\n",
    "#    for key, value in tqdm(in_out_dict.items()):\n",
    "#        print(key)\n",
    "#        if IMG_CHANNELS > 3:\n",
    "#            [img_path, index_path, mask_path] = value\n",
    "#            X_train[n] = join_image_and_index(img_path, index_path)\n",
    "#        else:\n",
    "#            [img_path, mask_path] = value\n",
    "#            image = Image.open(img_path) # Using Pillow Image object\n",
    "#            image = image.resize((IMG_HEIGHT, IMG_WIDTH)) # \n",
    "#            X_train[n] = np.asarray(image)/255.\n",
    "#        mask = Image.open(mask_path) # Using Pillow Image object\n",
    "#        mask = mask.resize((IMG_HEIGHT, IMG_WIDTH))\n",
    "#        mask = np.array(mask)\n",
    "#        mask = mask.reshape(IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "#        mask_bool = mask > 0          \n",
    "#        Y_train[n] = mask_bool \n",
    "#        n += 1\n",
    "#    return X_train, Y_train\n",
    "#\n",
    "#X_train_in, Y_train_in = read_data(in_out_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================== Generate training, validation, and test file groups ==================== \n",
    "def generate_train_val_test(X_train, Y_train):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size = 0.1)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 1/9)\n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test\n",
    "\n",
    "outfile = json.load(open('config.json'))['filepaths']['default_pyobjects'] + '/tile_filepath_splits_HS.npz'\n",
    "if not os.path.exists(outfile):\n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test = generate_train_val_test(X_in, Y_in)    \n",
    "    np.savez(outfile, X_train=X_train, Y_train=Y_train, X_val=X_val, Y_val=Y_val, X_test=X_test, Y_test=Y_test)\n",
    "    npzfile = np.load(outfile)\n",
    "else:\n",
    "    npzfile = np.load(outfile)\n",
    "    X_train = npzfile['X_train']\n",
    "    Y_train = npzfile['Y_train']\n",
    "    X_test = npzfile['X_test']\n",
    "    Y_test = npzfile['Y_test']\n",
    "    X_val = npzfile['X_val']\n",
    "    Y_val = npzfile['Y_val']\n",
    "\n",
    "print(\"Train Size = {0}, Test size = {1}, Validation size = {2}\".format(len(X_train), len(X_test), len(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Build data generators ====================\n",
    "train_generator = Batch_Generator(image_filenames=npzfile['X_train'], mask_filenames=npzfile['Y_train'], batch_size=1,\n",
    "                                image_height=IMG_HEIGHT, image_width=IMG_WIDTH, image_channels=IMG_CHANNELS)\n",
    "val_generator = Batch_Generator(image_filenames=npzfile['X_val'], mask_filenames=npzfile['Y_val'], batch_size=1,\n",
    "                                image_height=IMG_HEIGHT, image_width=IMG_WIDTH, image_channels=IMG_CHANNELS)\n",
    "test_generator = Batch_Generator(image_filenames=npzfile['X_test'], mask_filenames=npzfile['Y_test'], batch_size=1,\n",
    "                                image_height=IMG_HEIGHT, image_width=IMG_WIDTH, image_channels=IMG_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================== Train UNetS Model ====================      \n",
    "\n",
    "unets_model = UNetS(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "\n",
    "callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='logs'),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=json.load(open('config.json'))['filepaths']['default_checkpoints']+'/'+IMG_INDEX+'/UNetS-SVANN(HS).h5', verbose=1, save_best_only=True)]\n",
    "\n",
    "results = unets_model.fit(x=train_generator, validation_data=val_generator, epochs=8, workers=64, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================== Train UNet Model ====================\n",
    "\n",
    "unet_model = UNet(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "\n",
    "callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='logs'),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=json.load(open('config.json'))['filepaths']['default_checkpoints']+'/'+IMG_INDEX+'/UNet-SVANN(HS).h5', verbose=1, save_best_only=True)]\n",
    "\n",
    "results = unet_model.fit(x=train_generator, validation_data=val_generator, epochs=8, workers=8, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Train SegNet Model ====================\n",
    "\n",
    "segnet_model = SegNet((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), 1, 3)\n",
    "\n",
    "callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='logs'),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=json.load(open('config.json'))['filepaths']['default_checkpoints']+'/'+IMG_INDEX+'/SegNet-SVANN(HS).h5', verbose=1, save_best_only=True)]\n",
    "\n",
    "results = segnet_model.fit(x=train_generator, validation_data=val_generator, epochs=8, workers=8, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Build X and Y sets for test and validation ====================\n",
    "print('\\r', 'Building the validation labels...')\n",
    "Y_val = np.empty((0, IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.uint8)\n",
    "X_val = np.empty((0, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n",
    "for X_batch, Y_batch in tqdm(val_generator):\n",
    "    X_val = np.append(X_val, X_batch*255., axis=0)\n",
    "    Y_val = np.append(Y_val, Y_batch, axis=0)\n",
    "\n",
    "print('\\r', 'Building the test labels...')\n",
    "Y_test = np.empty((0, IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.uint8)\n",
    "X_test = np.empty((0, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n",
    "for X_batch, Y_batch in tqdm(test_generator):\n",
    "    X_test = np.append(X_test, X_batch*255., axis=0)\n",
    "    Y_test = np.append(Y_test, Y_batch, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================== Run model predictions ====================\n",
    "MODEL = 'SegNet'\n",
    "\n",
    "unet_model = UNet(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "weight_file = json.load(open('config.json'))['filepaths']['default_checkpoints']+'/'+IMG_INDEX+'/UNet-SVANN(HS).h5'\n",
    "unet_model.load_weights(weight_file)\n",
    "\n",
    "unets_model = UNetS(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "weight_file = json.load(open('config.json'))['filepaths']['default_checkpoints']+'/'+IMG_INDEX+'/UNetS-SVANN(HS).h5'\n",
    "unets_model.load_weights(weight_file)\n",
    "\n",
    "segnet_model = SegNet((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), 1, 3)\n",
    "weight_file = json.load(open('config.json'))['filepaths']['default_checkpoints']+'/'+IMG_INDEX+'/SegNet-SVANN(HS).h5'\n",
    "segnet_model.load_weights(weight_file)\n",
    "\n",
    "print('\\r', 'Predicting on validation data...')\n",
    "if MODEL == 'UNet':\n",
    "    preds_val = np.squeeze(unet_model.predict(val_generator, verbose=1, workers=64))\n",
    "elif MODEL == 'UNetS':\n",
    "    preds_val = np.squeeze(unets_model.predict(val_generator, verbose=1, workers=64))\n",
    "elif MODEL == 'SegNet':\n",
    "    preds_val = np.squeeze(segnet_model.predict(val_generator, verbose=1, workers=64))\n",
    "\n",
    "print('\\r', 'Predicting on test data...')\n",
    "if MODEL == 'UNet':\n",
    "    preds_test = np.squeeze(unet_model.predict(test_generator, verbose=1, workers=64))\n",
    "elif MODEL == 'UNetS':\n",
    "    preds_test = np.squeeze(unets_model.predict(test_generator, verbose=1, workers=64))\n",
    "elif MODEL == 'SegNet':\n",
    "    preds_test = np.squeeze(segnet_model.predict(test_generator, verbose=1, workers=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Run val and test evaluations ====================\n",
    "for model in ['UNet', 'UNetS', 'SegNet']:\n",
    "    print('Evaluating for %s...'%(model))\n",
    "    if model == 'UNet':\n",
    "        score, acc = unet_model.evaluate(val_generator, workers=64)\n",
    "    elif model == 'UNetS':\n",
    "        score, acc = unets_model.evaluate(val_generator, workers=64)\n",
    "    elif model == 'SegNet':\n",
    "        score, acc = segnet_model.evaluate(val_generator, workers=64)\n",
    "    print('Validation score:', score)\n",
    "    print('Validation accuracy:', acc)\n",
    "\n",
    "    if model == 'UNet':\n",
    "        score, acc = unet_model.evaluate(test_generator, workers=64)\n",
    "    elif model == 'UNetS':\n",
    "        score, acc = unets_model.evaluate(test_generator, workers=64)\n",
    "    elif model == 'SegNet':\n",
    "        score, acc = segnet_model.evaluate(test_generator, workers=64)\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Perform a sanity check on some random training samples ====================\n",
    "#for ix in range(len(preds_train_t[0:5])):\n",
    "#    plt.ioff()\n",
    "#    fig, axs = plt.subplots(2, 3, figsize=(16, 10), squeeze=True)\n",
    "#    fig.suptitle('%s with threshold: %0.3f' % (IMG_INDEX, best_threshold))\n",
    "#    axs[0, 0].imshow(X_train[ix][:,:,:3])\n",
    "#    axs[0, 0].set_title(\"Input imagery\")\n",
    "#    if IMG_CHANNELS > 3:\n",
    "#        axs[0, 1].imshow(X_train[ix][:,:,3:4])\n",
    "#        axs[0, 1].set_title(\"Input index\")\n",
    "#    axs[0, 2].imshow(Y_train[ix])\n",
    "#    axs[0, 2].set_title(\"Binary mask label\")\n",
    "#    axs[1, 0].imshow(preds_train[ix])\n",
    "#    axs[1, 0].set_title('Prediction values')\n",
    "#    axs[1, 1].imshow(preds_train_t[ix])\n",
    "#    axs[1, 1].set_title('Binary prediction')\n",
    "#    cm = np.squeeze(Y_train[ix]).astype(int)-2*np.squeeze(preds_train_t[ix]).astype(int)\n",
    "#    tn_, fp_, fn_, tp_ = confusion_matrix(Y_train[ix].astype(int).flatten(), preds_train_t[ix].astype(int).flatten(), labels=[0,1]).ravel()\n",
    "#    precision = tp_/(tp_+fp_)\n",
    "#    recall = tp_/(tp_+fn_)\n",
    "#    f1 = 2*(precision*recall)/(precision+recall)\n",
    "#    axs[1, 2].imshow(cm, cmap='RdBu', interpolation='gaussian')\n",
    "#    axs[1, 2].set_title('Confusion Matrix (f1: %0.3f)\\nLight Red=tp, Light Blue=tn, Red=fp, Blue=fn' % (f1))\n",
    "#    #plt.savefig('outputs/plots/'+str(ix)+'/UNetS_'+IMG_INDEX+'_'+str(ix))\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Calculate optimal prediction threshold ====================\n",
    "\n",
    "# ROC Curve\n",
    "print('Calculating ROC curve...')\n",
    "ns_probs = np.zeros(Y_val.shape)\n",
    "ns_fpr, ns_tpr, _ = roc_curve(Y_val.flatten().astype(np.uint8), ns_probs.flatten())\n",
    "fpr, tpr, roc_thresholds = roc_curve(Y_val.flatten().astype(np.uint8), preds_val.flatten())\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "ix = np.argmax(gmeans)\n",
    "best_roc_threshold = roc_thresholds[ix]\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (best_roc_threshold, gmeans[ix]))\n",
    "plt.plot(fpr, tpr, label='Logistic')\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "plt.xlabel('False Negative Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision Recall Curve\n",
    "#print('Calculating Precision/Recall curve...')\n",
    "#precision, recall, pr_thresholds = precision_recall_curve(Y_val.flatten().astype(np.uint8), preds_val.flatten())\n",
    "#fscore = (2 * precision * recall) / (precision + recall + 0.01)\n",
    "#ix = np.argmax(fscore)\n",
    "#best_pr_threshold = pr_thresholds[ix]\n",
    "#print('Best Threshold=%f, F-Score=%.3f' % (best_pr_threshold, fscore[ix]))\n",
    "#no_skill = len(Y_val.flatten()[Y_val.flatten()==1]) / len(Y_val.flatten())\n",
    "#plt.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "#plt.plot(recall, precision, label='Logistic')\n",
    "#plt.scatter(recall[ix], precision[ix], marker='o', color='black', label='Best')\n",
    "#plt.xlabel('Recall')\n",
    "#plt.ylabel('Precision')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = round(best_roc_threshold, 3)\n",
    "#best_threshold = round(best_pr_threshold, 3)\n",
    "print(best_threshold)\n",
    "preds_val_t = (preds_val > best_threshold).astype(np.uint8)\n",
    "preds_test_t = (preds_test > best_threshold).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Sanity check - validation ====================\n",
    "ix = random.randint(0, len(X_val)-1)\n",
    "imshow(X_val[ix][:,:,:3])\n",
    "plt.title(\"Input imagery\")\n",
    "plt.show()\n",
    "print(\"Mask coverage: %0.3f\"%(np.mean(Y_val[ix])))\n",
    "imshow(np.squeeze(Y_val[ix]))\n",
    "plt.title(\"Binary mask label\")\n",
    "plt.show()\n",
    "print(preds_val_t[ix])\n",
    "imshow(np.squeeze(preds_val[ix]))\n",
    "plt.title('Prediction Values')\n",
    "plt.show()\n",
    "imshow(np.squeeze(preds_val_t[ix]))\n",
    "plt.title('Binary prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Sanity check - test ====================\n",
    "ix = random.randint(0, len(X_test)-1)\n",
    "imshow(X_test[ix][:,:,:3])\n",
    "plt.title(\"Input imagery\")\n",
    "plt.show()\n",
    "print(\"Mask coverage: %0.3f\"%(np.mean(Y_test[ix])))\n",
    "imshow(np.squeeze(Y_test[ix]))\n",
    "plt.title(\"Binary mask label\")\n",
    "plt.show()\n",
    "imshow(np.squeeze(preds_test_t[ix]))\n",
    "plt.title('Binary prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================== Confusion matrix - test ====================\n",
    "tn = fp = fn = tp = 0.\n",
    "for ix in tqdm(range(len(preds_test_t))):\n",
    "    tn_, fp_, fn_, tp_ = confusion_matrix(Y_test[ix].astype(int).flatten(), preds_test_t[ix].astype(int).flatten(), labels=[0,1]).ravel()\n",
    "    tn = tn + tn_\n",
    "    fp = fp + fp_\n",
    "    fn = fn + fn_\n",
    "    tp = tp + tp_\n",
    "\n",
    "print('\\n', tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================== Confusion matrix - validation ====================\n",
    "tn = fp = fn = tp = 0.\n",
    "for ix in tqdm(range(len(preds_val_t))):\n",
    "    tn_, fp_, fn_, tp_ = confusion_matrix(Y_val[ix].astype(int).flatten(), preds_val_t[ix].astype(int).flatten(), labels=[0,1]).ravel()\n",
    "    tn = tn + tn_\n",
    "    fp = fp + fp_\n",
    "    fn = fn + fn_\n",
    "    tp = tp + tp_\n",
    "    \n",
    "print('\\n', tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selecting test data samples\n",
    "# Need to run once at the start\n",
    "# print(img_mask_HCL_pair.keys)\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# TEST_HCL_PATH = '/Users/jayantgupta/Desktop/Imagery/Wetland_Imagery/027_23_05_020/Test/img'\n",
    "# TEST_HCL_MASK_PATH = '/Users/jayantgupta/Desktop/Imagery/Wetland_Imagery/027_23_05_020/Test/mask'\n",
    "\n",
    "# TEST_HCR_PATH = '/Users/jayantgupta/Desktop/Imagery/Wetland_Imagery/027_23_05_021/Test/img'\n",
    "# TEST_HCR_MASK_PATH = '/Users/jayantgupta/Desktop/Imagery/Wetland_Imagery/027_23_05_021/Test/mask'\n",
    "\n",
    "# test_samples_HCL = random.choices(list(img_mask_HCL_pair.keys()), k=98)\n",
    "# for sample in test_samples_HCL:\n",
    "#     try:\n",
    "#         img_path = img_mask_HCL_pair[sample][0]\n",
    "#         img_test_path = os.path.join(TEST_HCL_PATH,img_path.split('/')[-1])\n",
    "#         shutil.move(img_path, img_test_path)\n",
    "#     #print(img_path)\n",
    "#     #print(img_test_path)\n",
    "\n",
    "#         mask_path = img_mask_HCL_pair[sample][1]\n",
    "#         mask_test_path = os.path.join(TEST_HCL_MASK_PATH, mask_path.split('/')[-1])\n",
    "#         shutil.move(mask_path, mask_test_path)\n",
    "#     except:\n",
    "#         print(sample)\n",
    "#     #break;\n",
    "\n",
    "# test_samples_HCR = random.choices(list(img_mask_HCR_pair.keys()), k=98)\n",
    "# for sample in test_samples_HCR:\n",
    "#     try:\n",
    "#         img_path = img_mask_HCR_pair[sample][0]\n",
    "#         img_test_path = os.path.join(TEST_HCR_PATH,img_path.split('/')[-1])\n",
    "#         shutil.move(img_path, img_test_path)\n",
    "    \n",
    "#         mask_path = img_mask_HCR_pair[sample][1]\n",
    "#         mask_test_path = os.path.join(TEST_HCR_MASK_PATH, mask_path.split('/')[-1])\n",
    "#         shutil.move(mask_path, mask_test_path)    \n",
    "#     except:\n",
    "#         print(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}